{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1010175,"sourceType":"datasetVersion","datasetId":447547},{"sourceId":9985719,"sourceType":"datasetVersion","datasetId":6145213}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary libraries\n!pip install transformers datasets huggingface_hub accelerate\n\nimport os\nimport torch\nfrom huggingface_hub import login\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\n# Disable WandB logging by setting environment variables\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"  # Disable advisory warnings for transformers\n\n# Authenticate Hugging Face account (use your token here)\nos.environ[\"HF_HOME\"] = \"/root/.cache/huggingface\"  # Set Hugging Face cache location\ntoken = \"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\"  # Replace with your Hugging Face token\nlogin(token)\n\n# Check if CUDA (GPU) is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the Llama model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel.to(device)\n\n# Add a padding token if it doesn't exist\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))  # Resize embeddings to account for new token\n\n# Check total model parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params / 1e6:.2f} million\")\n\n# Load the SST-2 dataset for binary classification\ndataset = load_dataset(\"glue\", \"sst2\")\n\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Set the dataset format for PyTorch\ntokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# Split dataset into training and testing subsets\ntrain_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500))  # First 500 samples for training\ntest_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42).select(range(100))  # First 100 samples for testing\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",  \n    evaluation_strategy=\"epoch\",  \n    learning_rate=2e-5,  \n    per_device_train_batch_size=1,  # Small batch size to avoid OOM errors\n    per_device_eval_batch_size=1,   \n    gradient_accumulation_steps=8,  # Accumulate gradients for larger effective batch size\n    num_train_epochs=3,  # Reduce epochs for quicker training\n    weight_decay=0.01,  \n    seed=1,\n    logging_dir=\"./logs\",  \n    logging_strategy=\"epoch\",  \n    run_name=\"llama3_finetuning\",  # Custom name for logging\n    fp16=True  # Enable mixed precision training\n)\n\n# Initialize the trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer\n)\n\n# Fine-tune the model\nprint(\"Starting training...\")\ntrainer.train()\n\n# Evaluate the model\nprint(\"Evaluating the model...\")\nresults = trainer.evaluate()\n\n# Print evaluation results\nprint(\"Evaluation Results:\")\nprint(results)\n\n# Save the fine-tuned model\nmodel_dir = \"./fine-tuned-llama\"\nmodel.save_pretrained(model_dir)\ntokenizer.save_pretrained(model_dir)\nprint(f\"Model and tokenizer saved to {model_dir}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predictions\n!pip install transformers datasets huggingface_hub accelerate\n\nimport os\nimport torch\nfrom huggingface_hub import login\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport pandas as pd\nfrom datasets import Dataset, load_dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    AutoModelForSequenceClassification, \n    TrainingArguments, \n    Trainer,\n    EarlyStoppingCallback,\n    DataCollatorWithPadding)\n\n\nimport numpy as np\n\nimport random\n\n\n# Disable WandB logging by setting environment variables\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"  # Disable advisory warnings for transformers\n\n# Authenticate Hugging Face account (use your token here)\nos.environ[\"HF_HOME\"] = \"/root/.cache/huggingface\"  # Set Hugging Face cache location\ntoken = \"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\"  # Replace with your Hugging Face token\nlogin(token)\n\n# Check if CUDA (GPU) is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the Llama model and tokenizer\nmodel_name = \"/kaggle/input/llama-sst-fine-tuned-model/fine-tuned-llama\" #load the pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel.to(device)\n\n# Add a padding token if it doesn't exist\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))  # Resize embeddings to account for new token\n\n# Check total model parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params / 1e6:.2f} million\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the SST-2 dataset for binary classification\ndataset = load_dataset(\"glue\", \"sst2\", split='train')\ndataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset=dataset.train_test_split(.2)\n# dataset['test']['label']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(input_text):\n    \"\"\"\n    Predicts the sentiment label for a given text input.\n\n    Args:\n        input_text (str): The text to predict the sentiment for.\n\n    Returns:\n        float: The predicted probability of the text being positive sentiment.\n    \"\"\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # Convert to PyTorch tensors and move to GPU (if available)\n    with torch.no_grad():\n        outputs = model(**inputs).logits  # Get the model's output logits\n    y_prob = torch.sigmoid(outputs).tolist()[0]  # Apply sigmoid activation and convert to list\n    return np.round(y_prob, 5)  # Round the predicted probability to 5 decimal places\ninput","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.DataFrame(dataset['test'])\ndf_test['prediction'] = df_test['sentence'].map(predict)\ndf_test['y_pred'] = df_test['prediction'].apply(lambda x: np.argmax(x, axis=0)) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Tokenize the dataset\ndef tokenize_function(examples):\n    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Set the dataset format for PyTorch\ntokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# Split dataset into training and testing subsets\ntrain_dataset = tokenized_datasets[\"train\"] #.shuffle(seed=42).select(range(500))  # First 500 samples for training\ntest_dataset = tokenized_datasets[\"test\"] #.shuffle(seed=42).select(range(100))  # First 100 samples for testing\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Calculate metrics for multiclass setting\naccuracy = accuracy_score(df_test['label'], df_test['y_pred'])\nprecision = precision_score(df_test['label'], df_test['y_pred'], average='weighted')\nrecall = recall_score(df_test['label'], df_test['y_pred'], average='weighted')\nf1 = f1_score(df_test['label'], df_test['y_pred'], average='weighted')\n\n# Print metrics\nprint(f\"Model Metrics on Test Data:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pre training\n# !pip install transformers datasets huggingface_hub accelerate\n\nimport os\nimport torch\nfrom huggingface_hub import login\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom datasets import load_dataset\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np  # Ensure numpy is imported\n\n# Disable WandB logging by setting environment variables\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"  # Disable advisory warnings for transformers\n\n# Authenticate Hugging Face account\nos.environ[\"HF_HOME\"] = \"/root/.cache/huggingface\"  # Set Hugging Face cache location\ntoken = \"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\"  # Replace with your Hugging Face token\nlogin(token)\n\n# Check if CUDA (GPU) is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the Llama model and tokenizer\nmodel_name = \"meta-llama/Llama-3.2-1B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel.to(device)\n\n# Add a padding token if it doesn't exist\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))  # Resize embeddings to account for new token\n\n# Check total model parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params / 1e6:.2f} million\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:20:47.222041Z","iopub.execute_input":"2024-11-22T21:20:47.222412Z","iopub.status.idle":"2024-11-22T21:20:56.040373Z","shell.execute_reply.started":"2024-11-22T21:20:47.222377Z","shell.execute_reply":"2024-11-22T21:20:56.039422Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Total parameters: 1235.82 million\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load the SST-2 dataset for binary classification\ndataset = load_dataset(\"glue\", \"sst2\", split='train')\ndataset = dataset.train_test_split(test_size=0.2)  # Split dataset into train/test\n\n# Define the prediction function\ndef predict(input_text):\n    \"\"\"\n    Predicts the sentiment label for a given text input.\n\n    Args:\n        input_text (str): The text to predict the sentiment for.\n\n    Returns:\n        float: The predicted probability of the text being positive sentiment.\n    \"\"\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Use defined device\n    with torch.no_grad():\n        outputs = model(**inputs).logits  # Get the model's output logits\n    y_prob = torch.softmax(outputs, dim=-1).tolist()[0]  # Use softmax for multi-class classification\n    return np.round(y_prob, 5)  # Round the predicted probabilities to 5 decimal places\n\n# Predict for the test set\ndf_test = pd.DataFrame(dataset['test'])\ndf_test['prediction'] = df_test['sentence'].map(predict)\ndf_test['y_pred'] = df_test['prediction'].apply(lambda x: np.argmax(x))  # Extract the predicted class\ndf_test['label'] = dataset['test']['label']  # Add ground truth labels\n\n# Calculate metrics for the test set\naccuracy = accuracy_score(df_test['label'], df_test['y_pred'])\nprecision = precision_score(df_test['label'], df_test['y_pred'], average='weighted')\nrecall = recall_score(df_test['label'], df_test['y_pred'], average='weighted')\nf1 = f1_score(df_test['label'], df_test['y_pred'], average='weighted')\n\n# Print metrics\nprint(f\"Model Metrics on Test Data:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Display sample predictions\nprint(df_test.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:20:56.041744Z","iopub.execute_input":"2024-11-22T21:20:56.042236Z","iopub.status.idle":"2024-11-22T21:25:20.700054Z","shell.execute_reply.started":"2024-11-22T21:20:56.042208Z","shell.execute_reply":"2024-11-22T21:25:20.699062Z"}},"outputs":[{"name":"stdout","text":"Model Metrics on Test Data:\nAccuracy: 0.4352\nPrecision: 0.7542\nRecall: 0.4352\nF1 Score: 0.2640\n                                            sentence  label    idx  \\\n0  maintain both a level of sophisticated intrigu...      1  22735   \n1                                   unsophisticated       0  44001   \n2             mr. scorsese 's bravery and integrity       1  28822   \n3                           spectacularly beautiful       1  45791   \n4  , parker exposes the limitations of his skill ...      0  54951   \n\n           prediction  y_pred  \n0  [0.98533, 0.01467]       0  \n1  [0.98213, 0.01787]       0  \n2    [0.9626, 0.0374]       0  \n3  [0.99514, 0.00486]       0  \n4  [0.92242, 0.07758]       0  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Post training\n# !pip install transformers datasets huggingface_hub accelerate\n\nimport os\nimport torch\nfrom huggingface_hub import login\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom datasets import load_dataset\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport numpy as np  # Ensure numpy is imported\n\n# Disable WandB logging by setting environment variables\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"true\"  # Disable advisory warnings for transformers\n\n# Authenticate Hugging Face account\nos.environ[\"HF_HOME\"] = \"/root/.cache/huggingface\"  # Set Hugging Face cache location\ntoken = \"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\"  # Replace with your Hugging Face token\nlogin(token)\n\n# Check if CUDA (GPU) is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the Llama model and tokenizer\nmodel_name = \"/kaggle/input/llama-sst-fine-tuned-model/fine-tuned-llama\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nmodel.to(device)\n\n# Add a padding token if it doesn't exist\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))  # Resize embeddings to account for new token\n\n# Check total model parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params / 1e6:.2f} million\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:25:20.701911Z","iopub.execute_input":"2024-11-22T21:25:20.702229Z","iopub.status.idle":"2024-11-22T21:25:23.636355Z","shell.execute_reply.started":"2024-11-22T21:25:20.702201Z","shell.execute_reply":"2024-11-22T21:25:23.635424Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nUsing device: cuda\nTotal parameters: 1235.82 million\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load the SST-2 dataset for binary classification\ndataset = load_dataset(\"glue\", \"sst2\", split='train')\ndataset = dataset.train_test_split(test_size=0.2)  # Split dataset into train/test\n\n# Define the prediction function\ndef predict(input_text):\n    \"\"\"\n    Predicts the sentiment label for a given text input.\n\n    Args:\n        input_text (str): The text to predict the sentiment for.\n\n    Returns:\n        float: The predicted probability of the text being positive sentiment.\n    \"\"\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Use defined device\n    with torch.no_grad():\n        outputs = model(**inputs).logits  # Get the model's output logits\n    y_prob = torch.softmax(outputs, dim=-1).tolist()[0]  # Use softmax for multi-class classification\n    return np.round(y_prob, 5)  # Round the predicted probabilities to 5 decimal places\n\n# Predict for the test set\ndf_test = pd.DataFrame(dataset['test'])\ndf_test['prediction'] = df_test['sentence'].map(predict)\ndf_test['y_pred'] = df_test['prediction'].apply(lambda x: np.argmax(x))  # Extract the predicted class\ndf_test['label'] = dataset['test']['label']  # Add ground truth labels\n\n# Calculate metrics for the test set\naccuracy = accuracy_score(df_test['label'], df_test['y_pred'])\nprecision = precision_score(df_test['label'], df_test['y_pred'], average='weighted')\nrecall = recall_score(df_test['label'], df_test['y_pred'], average='weighted')\nf1 = f1_score(df_test['label'], df_test['y_pred'], average='weighted')\n\n# Print metrics\nprint(f\"Model Metrics on Test Data:\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Display sample predictions\nprint(df_test.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T21:34:08.834558Z","iopub.execute_input":"2024-11-22T21:34:08.834838Z","iopub.status.idle":"2024-11-22T21:38:25.323313Z","shell.execute_reply.started":"2024-11-22T21:34:08.834813Z","shell.execute_reply":"2024-11-22T21:38:25.322331Z"}},"outputs":[{"name":"stdout","text":"Model Metrics on Test Data:\nAccuracy: 0.8850\nPrecision: 0.8862\nRecall: 0.8850\nF1 Score: 0.8852\n                                            sentence  label    idx  \\\n0  rivals the top japanese animations of recent v...      1  37334   \n1  fondly remembered in the endlessly challenging...      1  40739   \n2  a glorious spectacle like those d.w. griffith ...      1  44751   \n3  begins to drag two-thirds through , when the m...      0  44536   \n4      take as many drugs as the film 's characters       0  35696   \n\n         prediction  y_pred  \n0  [1e-05, 0.99999]       1  \n1        [0.0, 1.0]       1  \n2        [0.0, 1.0]       1  \n3        [1.0, 0.0]       0  \n4  [0.99997, 3e-05]       0  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}